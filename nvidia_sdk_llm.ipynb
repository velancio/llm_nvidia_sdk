{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "44swgJcNUl5j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Notes to run this notebook**\n",
        "1. If its being run in colab use a T4 with high RAM. It also runs without high RAM but after some use it will crash since the reranker uses CPU and not GPU.\n",
        "2. HF_TOKEN is the hugging face token thats in the secrets of the notebook and loaded from there.\n",
        "3. If for some reason FAISS serialized vectorstore file \"vecstore_full.pkl\" does not download or corrupted then use the downloaded processed documents files \"documents_full_sdk.pkl\", \"documents_full_blogs.pkl\" and \"documents_full_forums.pkl\". Then perform splitting and vectorization using the sentence transformer and create the vectorstore."
      ],
      "metadata": {
        "id": "hZJ19WashxWo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "mbQYmlUMqJoM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup LLM chain"
      ],
      "metadata": {
        "id": "F28kfXLTj1B6"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZVnj_mJC4NPn"
      },
      "source": [
        "### Installing Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DFwTFWep4R0S",
        "outputId": "ea8d4d53-c181-49ab-d8fa-22a3ef36d437"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.27.2)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.38.1)\n",
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.10/dist-packages (0.15.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (23.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.2.0)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.20.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.2)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (4.10.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.3)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate) (12.3.101)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.10/dist-packages (0.42.0)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (0.7.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (1.11.4)\n",
            "Requirement already satisfied: numpy<1.28.0,>=1.21.6 in /usr/local/lib/python3.10/dist-packages (from scipy->bitsandbytes) (1.25.2)\n",
            "Requirement already satisfied: xformers in /usr/local/lib/python3.10/dist-packages (0.0.24)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from xformers) (1.25.2)\n",
            "Requirement already satisfied: torch==2.2.0 in /usr/local/lib/python3.10/dist-packages (from xformers) (2.2.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0->xformers) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0->xformers) (4.10.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0->xformers) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0->xformers) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0->xformers) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0->xformers) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0->xformers) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0->xformers) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0->xformers) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0->xformers) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0->xformers) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0->xformers) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0->xformers) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0->xformers) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0->xformers) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0->xformers) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0->xformers) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0->xformers) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.2.0->xformers) (12.3.101)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.2.0->xformers) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.2.0->xformers) (1.3.0)\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (0.1.10)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.27)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.9.3)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.6.4)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.33)\n",
            "Requirement already satisfied: langchain-community<0.1,>=0.0.25 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.0.25)\n",
            "Requirement already satisfied: langchain-core<0.2,>=0.1.28 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.1.28)\n",
            "Requirement already satisfied: langchain-text-splitters<0.1,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.0.1)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.1.13)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.25.2)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.6.3)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (3.21.0)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (0.9.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain) (2.4)\n",
            "Requirement already satisfied: anyio<5,>=3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2,>=0.1.28->langchain) (3.7.1)\n",
            "Requirement already satisfied: packaging<24.0,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2,>=0.1.28->langchain) (23.2)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.0->langchain) (3.9.15)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.3 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (2.16.3)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (4.10.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2024.2.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.28->langchain) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.28->langchain) (1.2.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (1.0.0)\n",
            "Requirement already satisfied: faiss-gpu in /usr/local/lib/python3.10/dist-packages (1.7.2)\n",
            "Requirement already satisfied: sentence_transformers in /usr/local/lib/python3.10/dist-packages (2.5.1)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.32.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (4.38.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (4.66.2)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (2.2.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.25.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.11.4)\n",
            "Requirement already satisfied: huggingface-hub>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (0.20.3)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (9.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (3.13.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (2.31.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (4.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (23.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (3.1.3)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.11.0->sentence_transformers) (12.3.101)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.32.0->sentence_transformers) (2023.12.25)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.32.0->sentence_transformers) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.32.0->sentence_transformers) (0.4.2)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence_transformers) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence_transformers) (3.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence_transformers) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->sentence_transformers) (1.3.0)\n",
            "Requirement already satisfied: nest_asyncio in /usr/local/lib/python3.10/dist-packages (1.6.0)\n",
            "Requirement already satisfied: rank_bm25 in /usr/local/lib/python3.10/dist-packages (0.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rank_bm25) (1.25.2)\n",
            "Requirement already satisfied: flashrank in /usr/local/lib/python3.10/dist-packages (0.1.66)\n",
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.10/dist-packages (from flashrank) (0.15.2)\n",
            "Requirement already satisfied: onnxruntime in /usr/local/lib/python3.10/dist-packages (from flashrank) (1.17.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from flashrank) (1.25.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from flashrank) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from flashrank) (4.66.2)\n",
            "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.10/dist-packages (from onnxruntime->flashrank) (15.0.1)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime->flashrank) (23.5.26)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from onnxruntime->flashrank) (23.2)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from onnxruntime->flashrank) (3.20.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime->flashrank) (1.12)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->flashrank) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->flashrank) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->flashrank) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->flashrank) (2024.2.2)\n",
            "Requirement already satisfied: huggingface_hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from tokenizers->flashrank) (0.20.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers->flashrank) (3.13.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers->flashrank) (2023.6.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers->flashrank) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers->flashrank) (4.10.0)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.10/dist-packages (from coloredlogs->onnxruntime->flashrank) (10.0)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime->flashrank) (1.3.0)\n",
            "Requirement already satisfied: gdown in /usr/local/lib/python3.10/dist-packages (4.7.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown) (3.13.1)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.10/dist-packages (from gdown) (2.31.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from gdown) (1.16.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from gdown) (4.66.2)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown) (4.12.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown) (2.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2024.2.2)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (1.7.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install accelerate transformers tokenizers\n",
        "!pip install bitsandbytes einops\n",
        "!pip install xformers\n",
        "!pip install langchain\n",
        "!pip install faiss-gpu\n",
        "!pip install sentence_transformers\n",
        "!pip install nest_asyncio\n",
        "!pip install rank_bm25\n",
        "!pip install flashrank\n",
        "!pip install gdown"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load the model"
      ],
      "metadata": {
        "id": "sYErcFgOjwBk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_6jtxgdY7X-Z",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67,
          "referenced_widgets": [
            "3c0d074fff844a2b8fb7568dcfe62c40",
            "21244bf9935c4dae9d72b12af10522f2",
            "fc0a5a0f773548d593df0fcf88a279e3",
            "47c0a3a39c8d402aa72a44c6b0b1be0f",
            "bef30a56c8474246b8c666bc1186c3ba",
            "f47bcd669bb14efca6616f6e08d0db66",
            "5bae4b8e70cb4db9a82a222ca8323be6",
            "229ac37ba865425da35d62723787aadd",
            "cf5554ecca14424b8296d33926e1b61a",
            "61d341f3cc084c77ae8685eb1341a3ef",
            "dcdd2fcef1a9411ebe2b335fdc7c56e6"
          ]
        },
        "outputId": "c5c1463b-8a3f-4924-ed15-3a2f9066506f"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3c0d074fff844a2b8fb7568dcfe62c40"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded on cuda:0\n"
          ]
        }
      ],
      "source": [
        "from torch import cuda, bfloat16, float16\n",
        "import transformers\n",
        "from google.colab import userdata\n",
        "\n",
        "\n",
        "model_id = 'meta-llama/Llama-2-7b-chat-hf'\n",
        "\n",
        "device = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'\n",
        "\n",
        "# set quantization configuration to load large model with less GPU memory\n",
        "# this requires the `bitsandbytes` library\n",
        "bnb_config = transformers.BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type='nf4',\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_compute_dtype=bfloat16\n",
        ")\n",
        "\n",
        "# begin initializing HF items, you need an access token\n",
        "hf_auth = userdata.get('HF_TOKEN')\n",
        "model_config = transformers.AutoConfig.from_pretrained(\n",
        "    model_id,\n",
        "    token=hf_auth\n",
        ")\n",
        "\n",
        "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    token=hf_auth,\n",
        "    trust_remote_code=True,\n",
        "    config=model_config,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map='auto'\n",
        ")\n",
        "\n",
        "# enable evaluation mode to allow model inference\n",
        "model.eval()\n",
        "\n",
        "print(f\"Model loaded on {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load the model tokenizer"
      ],
      "metadata": {
        "id": "zco0N-FAjsnY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UgB6X146-rdt"
      },
      "outputs": [],
      "source": [
        "tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
        "    model_id,\n",
        "    token=hf_auth\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define the stopping criteria for the llm"
      ],
      "metadata": {
        "id": "kOcDyMZUjiQ4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MlMZapv5-wR-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b5493ea4-4b21-4b8a-c4a5-2747d62da98b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[1, 29871, 13, 29950, 7889, 29901], [1, 29871, 13, 28956, 13]]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "stop_list = ['\\nHuman:', '\\n```\\n']\n",
        "\n",
        "stop_token_ids = [tokenizer(x)['input_ids'] for x in stop_list]\n",
        "stop_token_ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0BGW598z-1Np",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e6e9f8e3-ee68-42ca-cf3d-5213d1b6b283"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[tensor([    1, 29871,    13, 29950,  7889, 29901], device='cuda:0'),\n",
              " tensor([    1, 29871,    13, 28956,    13], device='cuda:0')]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "stop_token_ids = [torch.LongTensor(x).to(device) for x in stop_token_ids]\n",
        "stop_token_ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eTfyJN2s-5eg"
      },
      "outputs": [],
      "source": [
        "from transformers import StoppingCriteria, StoppingCriteriaList\n",
        "\n",
        "# define custom stopping criteria object\n",
        "class StopOnTokens(StoppingCriteria):\n",
        "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n",
        "        for stop_ids in stop_token_ids:\n",
        "            if torch.eq(input_ids[0][-len(stop_ids):], stop_ids).all():\n",
        "                return True\n",
        "        return False\n",
        "\n",
        "stopping_criteria = StoppingCriteriaList([StopOnTokens()])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define the Hugging Face pipeline chain with the model and tokenizer"
      ],
      "metadata": {
        "id": "-_QjvFGjjTg4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uqj_HIrPCyDH"
      },
      "outputs": [],
      "source": [
        "\n",
        "generate_text = transformers.pipeline(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    return_full_text=True,  # langchain expects the full text\n",
        "    task='text-generation',\n",
        "    # we pass model parameters here too\n",
        "    stopping_criteria=stopping_criteria,  # without this model rambles during chat\n",
        "    max_new_tokens=1024,  # max number of tokens to generate in the output\n",
        "    repetition_penalty=1.1  # without this output begins repeating\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Initialize Hugging Face pipeline chain"
      ],
      "metadata": {
        "id": "9H6UluHXjINX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ONURnxt5FkGB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 270
        },
        "outputId": "fcb773d9-9414-46ad-c800-ae2fa1da5770"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.7 and will be removed in 0.2.0. Use invoke instead.\n",
            "  warn_deprecated(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n pa≈∫dziernik 12, 2022 No Comments by admin\\nNVIDIA CUDA (Compute Unified Device Architecture) is a parallel computing platform and programming model developed by NVIDIA for general-purpose computing on graphics processing units (GPUs). The CUDA Toolkit is a collection of software development tools and libraries that allow developers to use the GPU to accelerate their applications. It provides a set of programming APIs, sample codes, and documentation to help developers take advantage of the massively parallel processing capabilities of NVIDIA GPUs.\\nThe CUDA Toolkit includes several components:\\nCUDA Runtime: This is the runtime environment that manages the communication between the host CPU and the GPU. It provides a set of functions for launching kernel code on the GPU, handling data transfer between the GPU and CPU, and managing the memory hierarchy.\\nCUDA Samples: These are a set of pre-built examples that demonstrate how to use various CUDA features, such as thread blocking, shared memory, and synchronization.\\nCUDA Libraries: These are a set of libraries that provide additional functionality beyond what is provided by the CUDA Runtime. Examples include libraries for image processing, linear algebra, and machine learning.\\nCUDA Visual Profiler: This is a tool for profiling and analyzing the performance of CUDA kernels. It allows developers to identify bottlenecks in their code and optimize for better performance.\\nCUDA SDK: This is a software development kit (SDK) that provides a set of tools and libraries for developing CUDA applications. It includes the CUDA Runtime, samples, and libraries, as well as additional tools for debugging and testing.\\nCUDA Host Applications: These are applications that run on the host CPU and communicate with the GPU using the CUDA Runtime. They can be used to develop a wide range of applications, from scientific simulations to gaming.\\nCUDA GPU Accelerator: This is a hardware component that provides the actual processing power for CUDA applications. It consists of a GPU with a large number of cores, which can be used to execute multiple threads simultaneously.\\nOverall, the CUDA Toolkit provides a powerful platform for developers to harness the power of GPU acceleration for their applications. By leveraging the massively parallel processing capabilities of NVIDIA GPUs, developers can achieve faster performance, lower power consumption, and improved scalability for their applications.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "from langchain.llms import HuggingFacePipeline\n",
        "\n",
        "llm = HuggingFacePipeline(pipeline=generate_text)\n",
        "\n",
        "# checking again that everything is working fine\n",
        "llm(prompt=\"What is the NVIDIA CUDA Toolkit?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load embeddings"
      ],
      "metadata": {
        "id": "0tieAjP9jDbK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G3Va4oRbF7WZ"
      },
      "outputs": [],
      "source": [
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "\n",
        "model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "model_kwargs = {\"device\": \"cuda\"}\n",
        "\n",
        "embeddings = HuggingFaceEmbeddings(model_name=model_name, model_kwargs=model_kwargs)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "1CaODv64qesb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Vectorization of documents   \n",
        "*(Load the all documents files \"documents_full_sdk.pkl\", \"documents_full_blogs.pkl\" and \"documents_full_forums.pkl\" attached in the zip if vectorstore file \"vecstore_full.pkl\" is not available)*"
      ],
      "metadata": {
        "id": "xRXjEBdXX3xT"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f81D_78WTjfT"
      },
      "source": [
        "### Load scraped documents"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Download all shared scraped documents files \"documents_full_sdk.pkl\", \"documents_full_blogs.pkl\" and \"documents_full_forums.pkl\""
      ],
      "metadata": {
        "id": "crp4g-2MXqoP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown --id 1Iin64B6MwA34APl7Rt_pKFHPZC7qLreD\n",
        "!gdown --id 1PmNWDXsKV7aaA2ybmxUdPsAyaCSZQcfP\n",
        "!gdown --id 1v-H54VVZhwSighkVv3uaT8ki0SRFBhhl"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ty2pdwTpWXih",
        "outputId": "7fa3c294-c5a8-4d50-8fe6-e638aa442ff9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gdown/cli.py:138: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1Iin64B6MwA34APl7Rt_pKFHPZC7qLreD\n",
            "From (redirected): https://drive.google.com/uc?id=1Iin64B6MwA34APl7Rt_pKFHPZC7qLreD&confirm=t&uuid=c426a3e5-ce11-445c-aa8c-66210490ff5c\n",
            "To: /content/documents_full_sdk.pkl\n",
            "100% 243M/243M [00:01<00:00, 222MB/s]\n",
            "/usr/local/lib/python3.10/dist-packages/gdown/cli.py:138: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1PmNWDXsKV7aaA2ybmxUdPsAyaCSZQcfP\n",
            "To: /content/documents_full_forums.pkl\n",
            "100% 26.5k/26.5k [00:00<00:00, 75.9MB/s]\n",
            "/usr/local/lib/python3.10/dist-packages/gdown/cli.py:138: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1v-H54VVZhwSighkVv3uaT8ki0SRFBhhl\n",
            "To: /content/documents_full_blogs.pkl\n",
            "100% 4.30M/4.30M [00:00<00:00, 275MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G-NKh7sVTqM8"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "\n",
        "sdk_data_path = \"documents_full_sdk.pkl\"\n",
        "with open(sdk_data_path, 'rb') as file:\n",
        "    documents_sdk = pickle.load(file)\n",
        "\n",
        "blogs_data_path = \"documents_full_blogs.pkl\"\n",
        "with open(blogs_data_path, 'rb') as file:\n",
        "    documents_blogs = pickle.load(file)\n",
        "\n",
        "forums_data_path = \"documents_full_forums.pkl\"\n",
        "with open(forums_data_path, 'rb') as file:\n",
        "    documents_forums = pickle.load(file)\n",
        "\n",
        "documents = documents_sdk + documents_blogs + documents_forums"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Nx31Mao80Ij",
        "outputId": "346784af-fa0d-4bc2-d922-7b7d82270b84"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "page_content='NVIDIA Magnum IO Developer Environment | NVIDIA NGCNGC | CatalogWelcome GuestCatalogContainersNVIDIA Magnum IO Developer EnvironmentNVIDIA Magnum IO Developer EnvironmentFor copy image paths and more information, please view on a desktop device.DescriptionNVIDIA Magnum IO is the I/O technologies from NVIDIA and Mellanox that enable applications at scale. The Magnum IO Developer Environment container allows developers to begin scaling their applications on a laptop, desktop, workstation, or in the cloud.PublisherNVIDIALatest Tag21.07ModifiedFebruary 1, 2024Compressed Size3.8 GBMultinode SupportNoMulti-Arch SupportNo21.07 (Latest) Security Scan ResultsLinux / amd64Sorry, your browser does not support inline SVG. DL High Performance Computing High Performance Computing Infrastructure Software Multi-NodeOverviewTagsLayersSecurity ScanningRelated CollectionsNVIDIA Magnum IO Developer Environment\\nNVIDIA Magnum IO is the collection of I/O technologies from NVIDIA and\\nMellanox that make up the I/O subsystem of the modern data center, and\\nenable applications at scale. Making use of GPUS, or scaling an application\\nup to multiple GPUs, or scaling it out across multiple nodes, will probably\\nmake use of the libraries in Magnum IO.\\nThe Magnum IO Developer Environment container contains a comprehensive set\\nof tools to scale I/O. It serves two primary purposes 1) Allow developers to\\nbegin scaling applications on a laptop, desktop, workstation, or in the cloud.\\n2) Serve as the basis for a build container locally or in a CI/CD system.\\nQuick Links:\\n\\nMagnum IO Container on NGC\\nMagnum IO Code repo\\nReport issues\\n\\nQuick Start\\nThere are two ways to get the container and run it. With either option\\nthe system must be setup to run GPU enabled containers, which the\\ninstaller.sh script can do.\\nRecommended is to pull the NVIDIA NGC Catalog (option 1). If you plan to\\ncustomize the container further, it can be used as the FROM to build a new\\ncontainer. Also supported is building the container locally (option 2).\\nFor usage and command documentation: ./installer.sh help\\nOption 1: Pull container from NVIDIA NGC Catalog:\\n\\nSetup system\\n\\n# Clone repo\\ngit clone https://github.com/NVIDIA/MagnumIO.git\\ncd magnumio/dev-env\\n\\n# Setup system (driver, CUDA, Docker)\\n./installer.sh setup-system\\n\\n\\nVisit https://ngc.nvidia.com/catalog/containers/nvidia:magnum-io:magnum-io\\nand find the latest version.\\nPull\\n\\ndocker pull nvcr.io/nvidia/magnum-io/magnum-io:TAG\\n\\n\\nRun\\n\\ndocker run --gpus all --rm -it \\\\\\n  --user \"$(id -u):$(id -g)\" \\\\\\n  --volume $HOME:$HOME \\\\\\n  --volume /run/udev:/run/udev:ro \\\\\\n  --workdir $HOME \\\\\\n  magnum-io:TAG\\n\\nOption 2: Build the container locally:\\n# Clone repo\\ngit clone https://github.com/NVIDIA/MagnumIO.git\\ncd magnumio/dev-env\\n\\n# Setup system (driver, CUDA, Docker)\\n./installer.sh setup-system\\n\\n# Build the container\\n./installer.sh build-container\\n\\n#Run the container with HOME directory mounted\\n./installer.sh run-container\\n\\nUpgrading\\nThe installer.sh script is designed to detect old versions of dependencies\\nand upgrade them as needed. Simply git pull a new version, and run the\\nsystem-setup again.\\ngit pull\\n./installer.sh setup-system\\n\\nMinimum Hardware and Software\\nThe container should run and build code on any system with a GPU and minimal\\ndrivers enforced by the installer.sh script. This allows developers to\\nintegrate Magnum IO APIs easily, and get started on almost any system.\\nHowever some components of Magnum IO require specific hardware or\\nconfigurations to run applications with the APIs fully enabled.\\nFor example GDS requires the latest Tesla, Volta or Ampere GPUs, ext4\\nmounting options, and GDS enabled storage systems (an NVMe drive at minimum).\\nMore on GDS setup in the\\nNVIDIA GPUDirect Storage Installation and Troubleshooting Guide\\nIn practice, this means almost all development can be done on a local system,\\nmany tests can be run locally, but tests at scale are done in a cluster or\\ncloud environment.\\nMagnum IO Components\\nNCCL\\nThe\\nNVIDIA Collective Communications Library\\n(NCCL, pronounced ‚Äúnickel‚Äù)\\nis a library providing inter-GPU communication primitives that are\\ntopology-aware and can be easily integrated into applications.\\nNCCL is smart about I/O on systems with complex topology: systems with\\nmultiple CPUs, GPUs, PCI busses, and network interfaces. It can selectively\\nuse NVLink, Ethernet, and InfiniBand, using multiple links when possible.\\nConsider using NCCL APIs whenever you plan your application or library to\\nrun on a mix of multi-GPU multi-node systems in a data center, cloud, or\\nhybrid system. At runtime, NCCL determines the topology and optimizes\\nlayout and communication methods.\\nNVSHMEM\\nNVSHMEM creates a global address\\nspace for data that spans the memory of multiple GPUs and can be accessed\\nwith fine-grained GPU-initiated operations, CPU-initiated operations, and\\noperations on CUDA streams.\\nIn many HPC workflows, models and simulations are run that far exceed the\\nsize of a single GPU or node. NVSHMEM allows for a simpler asynchronous\\ncommunication model in a shared address space that spans GPUs within or\\nacross nodes, with lower overheads, possibly resulting in stronger scaling\\ncompared to a traditional Message Passing Interface (MPI).\\nUCX\\nUnified Communication X (UCX) uses\\nhigh-speed networks, including InfiniBand, for inter-node communication and\\nshared memory mechanisms for efficient intra-node communication.  If you\\nneed a standard CPU-driven MPI, PGAS OpenSHMEM libraries, and RPC, GPU-aware\\ncommunication is layered on top of UCX.\\nUCX is appropriate when driving I/O from the CPU, or when system memory is\\nbeing shared. UCX enables offloading the I/O operations to both host adapter\\n(HCA) and switch, which reduces CPU load. UCX simplifies the portability of\\nmany peer-to-peer operations in MPI systems.\\nGDS\\nNVIDIA GPUDirect Storage\\n(GDS) enables a direct data path for Remote Direct Memory Access (RDMA)\\ntransfers between GPU memory and storage, which avoids a bounce buffer and\\nmanagement by the CPU. This direct path increases system bandwidth and\\ndecreases the latency and utilization load on the CPU.\\nGDS and the cuFile APIs should be used whenever data needs to move directly\\nbetween storage and the GPU. With storage systems that support GDS,\\nsignificant increases in performance on clients are observed when I/O is a\\nbottleneck. In cases where the storage system does not support GDS, I/O\\ntransparently falls back to normal file reads and writes.\\nMoving the I/O decode/encode from the CPU to GPU creates new opportunities for\\ndirect data transfers between storage and GPU memory which can benefit from\\nGDS performance. An increasing number of data formats are supported in CUDA.\\nNSight\\nNVIDIA Nsight Systems lets you\\nsee what‚Äôs happening in the system and\\nNVIDIA Cumulus NetQ allows you\\nto analyze what‚Äôs happening on the NICs and switches.\\nBoth are critical to finding some causes of bottlenecks in multi-node\\napplications.\\nNsight Systems is a low-overhead performance analysis tool designed to\\nprovide insights that you need to optimize your software. It provides\\neverything that you would expect from a profiler for a GPU. Nsight Systems\\nhas a tight integration with many core CUDA libraries, giving you detailed\\ninformation on what is happening.\\nNsight Systems allows you to see exactly what‚Äôs happening on the system,\\nwhat code is taking a long time, and when algorithms are waiting on GPU/CPU\\ncompute, or device I/O. Nsight Systems is relevant to Magnum IO and included\\nin the Magnum IO container for convenience, but its scope spans well outside\\nof Magnum IO to monitoring compute that‚Äôs unrelated to I/O.\\nNetQ (not in container)\\nNetQ is a highly scalable, modern, network operations tool set that provides\\nvisibility, troubleshooting and lifecycle management of your open networks\\nin real time. It enables network profiling functionality that can be used\\nalong with Nsight Systems or application logs to observe the network‚Äôs\\nbehavior while the application is running.\\nNetQ is part of Magnum IO given its integral involvement in managing IO in\\naddition to profiling it, but is not inside the container as it runs on the\\nnodes and switches of the network.\\nOperating System Setup\\nDisable \"Secure Boot\" in the system BIOS/UEFI before installing Linux.\\nUbuntu\\nThe Data Science stacks are supported on Ubuntu LTS 18.04.1+ or 20.04\\nwith the 4.15+ kernel. Ubuntu can be downloaded from\\nhttps://www.ubuntu.com/download/desktop\\nSupport for Red Hat is planned in later releases.\\nInstalling the NVIDIA GPU Driver\\nIt is important that updated NVIDIA drivers are installed on the system.\\nThe minimum version of the NVIDIA driver supported is 460.39.\\nMore recent drivers may be available, and should work correctly.\\nUbuntu or RHEL v8.x Driver Install\\nDriver install for Ubuntu is handled by ./installer.sh setup-system\\nso no manual install should be required.\\nIf the driver if too old or the script is having problems, the driver can\\nbe removed (this may have side effects, read the warnings) and reinstalled:\\n./installer.sh purge-driver\\n# reboot\\n./installer.sh setup-system\\n# reboot\\n\\nLicense\\nThe NVIDIA Magnum IO Developer Environment is licensed under Apache 2.0 and contributions are accepted with a DCO. See the CONTRIBUTING.md for more information on how to contribute and the release artifacts.\\nSince the underlying images may include components licensed under open-source licenses such as GPL, the sources for these components are archived on the CUDA open source index. Source and licenses details are also available on the component websites.\\nMore Information\\n\\nNVIDIA Magnum IO\\nBlog: Optimizing Data Movement in GPU Applications with the NVIDIA Magnum IO Developer Environment\\n\\n' metadata={'source': 'https://catalog.ngc.nvidia.com/orgs/nvidia/teams/magnum-io/containers/magnum-io', 'title': 'NVIDIA Magnum IO Developer Environment | NVIDIA NGC', 'description': 'NVIDIA Magnum IO is the I/O technologies from NVIDIA and Mellanox that enable applications at scale. The Magnum IO Developer Environment container allows developers to begin scaling their applications on a laptop, desktop, workstation, or in the cloud.', 'language': 'en'}\n",
            "9905\n"
          ]
        }
      ],
      "source": [
        "print(documents[0])\n",
        "print(len(documents))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "kXxlS3Fvqkvp"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tAs6cza-T3I_"
      },
      "source": [
        "### Create document splits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MlnQ-o_pFwBr"
      },
      "outputs": [],
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=20)\n",
        "all_splits = text_splitter.split_documents(documents)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "mrVxD9sQqrfe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Vectorstore\n",
        "*(Load the vectorstore file \"vecstore_full.pkl\" attached in the zip)*"
      ],
      "metadata": {
        "id": "p6L4Ix_Y3ckd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### FAISS"
      ],
      "metadata": {
        "id": "3HzLDpzt3jZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Download shared vectorstore file \"vecstore_full.pkl\""
      ],
      "metadata": {
        "id": "8KOVje4yigzZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown --id 1fD7l9k7bHksE9htb7qbOMkzJahjJrK9B"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aIqwSBlgNcr_",
        "outputId": "919442bd-e6ac-4273-981a-9e289b41cbcf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gdown/cli.py:138: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1fD7l9k7bHksE9htb7qbOMkzJahjJrK9B\n",
            "From (redirected): https://drive.google.com/uc?id=1fD7l9k7bHksE9htb7qbOMkzJahjJrK9B&confirm=t&uuid=e08742e4-326e-4ba0-b4e7-dadac7e1248f\n",
            "To: /content/vecstore_full.pkl\n",
            "100% 493M/493M [00:04<00:00, 117MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Read from vectorstore"
      ],
      "metadata": {
        "id": "dlfvBkM0bdpj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VeSmfC8YUSnd"
      },
      "outputs": [],
      "source": [
        "import faiss\n",
        "import json\n",
        "import os\n",
        "import pickle\n",
        "from langchain.vectorstores import FAISS\n",
        "\n",
        "# Uncomment If loading from google drive\n",
        "# vecstore_data_path = \"/content/drive/MyDrive/datasets/llm/vecstore_full.pkl\"\n",
        "vecstore_data_path = \"vecstore_full.pkl\"\n",
        "if os.path.exists(vecstore_data_path):\n",
        "  with open(vecstore_data_path, 'rb') as f:\n",
        "    vecstore_data = pickle.load(f)\n",
        "    vectorstore = FAISS.deserialize_from_bytes(embeddings=embeddings, serialized=vecstore_data)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **[DO NOT RUN UNLESS NEEDED]** Write to vectorstore and Save"
      ],
      "metadata": {
        "id": "RtO_qutAbV9d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# storing embeddings in the vector store\n",
        "# vectorstore = FAISS.from_documents(all_splits, embeddings)"
      ],
      "metadata": {
        "id": "nB-acI3EdZiK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Write the vector store to disk\n",
        "'''vecstore_data_path = \"/content/drive/MyDrive/datasets/llm/vecstore_full_edited.pkl\"\n",
        "with open(vecstore_data_path, 'wb') as f:\n",
        "    # Assuming `docstore` is your document store object and it has a method to return its data as a dictionary\n",
        "    pickle.dump(vectorstore.serialize_to_bytes(), f)'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "id": "8vKFlv7LB-MO",
        "outputId": "e41c2135-c154-4e55-da02-85e1994f1d74"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'vecstore_data_path = \"/content/drive/MyDrive/datasets/llm/vecstore_full_edited.pkl\"\\nwith open(vecstore_data_path, \\'wb\\') as f:\\n    # Assuming `docstore` is your document store object and it has a method to return its data as a dictionary\\n    pickle.dump(vectorstore.serialize_to_bytes(), f)'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Rerank"
      ],
      "metadata": {
        "id": "Bgwrlw34HjJh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.retrievers.document_compressors import FlashrankRerank\n",
        "from langchain.retrievers import ContextualCompressionRetriever\n",
        "\n",
        "compressor = FlashrankRerank()\n",
        "compression_retriever = ContextualCompressionRetriever(\n",
        "    base_compressor=compressor, base_retriever=vectorstore.as_retriever(search_type=\"mmr\",search_kwargs={\"k\": 3})\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CnpdnbsmHlWJ",
        "outputId": "c5d1452f-13be-47f1-8255-af4debe94758"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading ms-marco-MultiBERT-L-12...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ms-marco-MultiBERT-L-12.zip: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 98.7M/98.7M [00:00<00:00, 136MiB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test code for Rerank compression retriever"
      ],
      "metadata": {
        "id": "kICp1ZCfL3em"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"How can I install the NVIDIA CUDA Toolkit on windows?\"\n",
        "docs = compression_retriever.get_relevant_documents(query=question)\n",
        "print(f\"length - {len(docs)}\")\n",
        "print(f\"{docs}\")\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_YX3hvGPL38N",
        "outputId": "38aa1840-d2ce-4d45-e6e3-15353b8e5201"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "length - 3\n",
            "[Document(page_content='CUDA Installation Guide for Microsoft Windows\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n1. Introduction\\n1.1. System Requirements\\n1.2. About This Document\\n\\n\\n2. Installing CUDA Development Tools\\n2.1. Verify You Have a CUDA-Capable GPU\\n2.2. Download the NVIDIA CUDA Toolkit\\n2.3. Install the CUDA Software\\n2.3.1. Uninstalling the CUDA Software\\n\\n\\n2.4. Using Conda to Install the CUDA Software\\n2.4.1. Conda Overview\\n2.4.2. Installation\\n2.4.3. Uninstallation\\n2.4.4. Installing Previous CUDA Releases\\n\\n\\n2.5. Use a Suitable Driver Model\\n2.6. Verify the Installation\\n2.6.1. Running the Compiled Examples\\n\\n\\n\\n\\n3. Pip Wheels\\n4. Compiling CUDA Programs\\n4.1. Compiling Sample Projects\\n4.2. Sample Projects\\n4.3. Build Customizations for New Projects\\n4.4. Build Customizations for Existing Projects\\n\\n\\n5. Additional Considerations\\n6. Notices\\n6.1. Notice\\n6.2. OpenCL\\n6.3. Trademarks\\n\\n\\n\\n\\n\\n\\n\\n\\nInstallation Guide Windows\\n\\n\\n\\n\\n\\n ¬ª\\n1. Introduction\\n\\n\\n\\nv12.3 |\\nPDF\\n|\\nArchive\\n\\xa0\\n\\n\\n\\n\\n\\n\\nCUDA Installation Guide for Microsoft Windows\\nThe installation instructions for the CUDA Toolkit on Microsoft Windows systems.\\n\\n1. Introduction\\uf0c1\\nCUDA¬Æ is a parallel computing platform and programming model invented by NVIDIA. It enables dramatic increases in computing performance by harnessing the power of the graphics processing unit (GPU).\\nCUDA was developed with several design goals in mind:', metadata={'id': 1, 'relevance_score': 0.9661605}), Document(page_content='CUDA Toolkit Archive | NVIDIA Developer\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nCUDA Toolkit Archive \\n\\nPrevious releases of the CUDA Toolkit, GPU Computing SDK, documentation and developer drivers can be found using the links below. Please select the release you want from the list below, and be sure to check www.nvidia.com/drivers for more recent production drivers appropriate for your hardware configuration.\\n\\n\\xa0\\nDownload Latest CUDA Toolkit\\nLearn More about CUDA Toolkit', metadata={'id': 2, 'relevance_score': 0.9660385}), Document(page_content='2.6. Choose an Installation Method\\uf0c1\\nThe CUDA Toolkit can be installed using either of two different installation mechanisms: distribution-specific packages (RPM and Deb packages), or a distribution-independent package (runfile packages).\\nThe distribution-independent package has the advantage of working across a wider set of Linux distributions, but does not update the distribution‚Äôs native package management system. The distribution-specific packages interface with the distribution‚Äôs native package management system. It is recommended to use the distribution-specific packages, where possible.\\n\\nNote\\nFor both native as well as cross development, the toolkit must be installed using the distribution-specific installer. See the CUDA Cross-Platform Installation section for more details.\\n\\n\\n\\n2.7. Download the NVIDIA CUDA Toolkit\\uf0c1\\nThe NVIDIA CUDA Toolkit is available at https://developer.nvidia.com/cuda-downloads.\\nChoose the platform you are using and download the NVIDIA CUDA Toolkit.\\nThe CUDA Toolkit contains the CUDA driver and tools needed to create, build and run a CUDA application as well as libraries, header files, and other resources.\\nDownload Verification\\nThe download can be verified by comparing the MD5 checksum posted at https://developer.download.nvidia.com/compute/cuda/12.3.2/docs/sidebar/md5sum.txt with that of the downloaded file. If either of the checksums differ, the downloaded file is corrupt and needs to be downloaded again.\\nTo calculate the MD5 checksum of the downloaded file, run the following:\\nmd5sum <file>', metadata={'id': 0, 'relevance_score': 0.9603605})]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "4luGYx1Yqt9r"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "885rQWfGEnef"
      },
      "source": [
        "## RetrievalQA chain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sw4bs8rUK6vG"
      },
      "outputs": [],
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "custom_template = \"\"\"\"\n",
        "[INST]<<SYS>>\n",
        "You are a helpful, respectful and honest assistant. Always answer as accurate as possible using the context text provided.\n",
        "Your answers should only answer the question once and not have any text after the answer is done.\n",
        "If you don't know the answer to a question, please don't share false information.\n",
        "If there is no context, just say you cannot answer the question and politely apologize.\n",
        "<</SYS>>\n",
        "\n",
        "CONTEXT:/n/n {context}/n\n",
        "\n",
        "Question: {question}[/INST]\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Fi7_Cod5NQS"
      },
      "outputs": [],
      "source": [
        "from langchain.chains import RetrievalQA\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "prompt = PromptTemplate(input_variables=[\"question\", \"context\"], template=custom_template)\n",
        "\n",
        "chain = RetrievalQA.from_chain_type(llm=llm, retriever=compression_retriever, return_source_documents=True, chain_type_kwargs={\"prompt\": prompt})"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "BLhlCFUEqyLa"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i18aiwvcGDJj"
      },
      "source": [
        "## Questions and Answers"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Helper wrappers to provide clean responses"
      ],
      "metadata": {
        "id": "pJvUEov5PZcr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import textwrap\n",
        "\n",
        "def wrap_text_preserve_newlines(text, width=110):\n",
        "    # Split the input text into lines based on newline characters\n",
        "    lines = text.split('\\n')\n",
        "\n",
        "    # Wrap each line individually\n",
        "    wrapped_lines = [textwrap.fill(line, width=width) for line in lines]\n",
        "\n",
        "    # Join the wrapped lines back together using newline characters\n",
        "    wrapped_text = '\\n'.join(wrapped_lines)\n",
        "\n",
        "    return wrapped_text\n",
        "\n",
        "def process_llm_response(llm_response):\n",
        "    print(wrap_text_preserve_newlines(llm_response['result']))\n",
        "    if \"source_documents\" in llm_response:\n",
        "      print(f'\\n\\nSources: {llm_response[\"source_documents\"]}')\n",
        "      for source in llm_response[\"source_documents\"]:\n",
        "        if 'source' in source.metadata:\n",
        "          print(source.metadata['source'])\n",
        "    torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "7J5k3AwpPYhh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example Queries"
      ],
      "metadata": {
        "id": "m2N3COCm5iXw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wQM5ihXUF3Ps",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "615bdeb1-9382-4c07-ad1d-3ef5521edf1f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
            "  warn_deprecated(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The NVIDIA CUDA Toolkit is a development environment for creating high-performance, GPU-accelerated\n",
            "applications. It provides a range of tools and libraries for developing, optimizing, and deploying\n",
            "applications on various hardware configurations, including desktop workstations, enterprise data centers,\n",
            "cloud-based platforms, and supercomputers. The toolkit includes GPU-accelerated libraries, debugging and\n",
            "optimization tools, a C/C++ compiler, and a runtime library.\n",
            "\n",
            "\n",
            "Sources: [Document(page_content='CUDA Toolkit - Free Tools and Training | NVIDIA Developer\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nCUDA Toolkit\\nThe NVIDIA¬Æ CUDA¬Æ Toolkit provides a development environment for creating high-performance, GPU-accelerated applications. With it, you can develop, optimize, and deploy your applications on GPU-accelerated embedded systems, desktop workstations, enterprise data centers, cloud-based platforms, and supercomputers. The toolkit includes GPU-accelerated libraries, debugging and optimization tools, a C/C++ compiler, and a runtime library.\\n\\n\\n\\n\\nThe Features of CUDA 12 \\n\\n\\n\\n\\nBuilt-In Capabilities for Easy Scaling\\nUsing built-in capabilities for distributing computations across multi-GPU configurations, you can develop applications that scale from single-GPU workstations to cloud installations with thousands of GPUs.\\n\\nLearn More \\n\\n\\n\\n\\n\\n\\nNew Release, New Benefits  \\nCUDA 12 introduces support for the NVIDIA Hopper‚Ñ¢ and Ada Lovelace architectures, Arm¬Æ server processors, lazy module and kernel loading, revamped dynamic parallelism APIs, enhancements to the CUDA graphs API, performance-optimized libraries, and new developer tool capabilities.\\n\\nLearn More \\n\\n\\n\\n\\n\\n\\nSupport for Hopper \\nSupport for the Hopper architecture includes next-generation Tensor Cores and Transformer Engine, the high-speed NVIDIA NVLink¬Æ Switch, mixed-precision modes, second-generation Multi-Instance GPU (MIG), advanced memory management, and standard C++/Fortran/Python parallel language constructs.\\n\\nLearn More \\n\\n\\n\\n\\n\\nDownload Now\\n\\n\\n\\n\\n\\nTutorials\\nCUDA Developer Tools is a series of tutorial videos designed to get you started using NVIDIA Nsight‚Ñ¢ tools for CUDA development. It explores key features for CUDA profiling, debugging, and optimizing.  \\n\\n\\n\\n\\n\\n\\n\\n\\nCUDA Compatibility \\nWatch Video \\n\\n\\n\\n\\n\\n\\n\\nCUDA Upgrades for Jetson Devices \\nWatch Video \\n\\n\\n\\n\\n\\n\\n\\nProfiling and Debugging Applications \\nWatch Video \\n\\n\\n\\n\\n\\n\\n\\nInstalling CUDA Toolkit on Windows and WSL \\nWatch Video', metadata={'id': 1, 'relevance_score': 0.97066104}), Document(page_content='CUDA Toolkit Archive | NVIDIA Developer\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nCUDA Toolkit Archive \\n\\nPrevious releases of the CUDA Toolkit, GPU Computing SDK, documentation and developer drivers can be found using the links below. Please select the release you want from the list below, and be sure to check www.nvidia.com/drivers for more recent production drivers appropriate for your hardware configuration.\\n\\n\\xa0\\nDownload Latest CUDA Toolkit\\nLearn More about CUDA Toolkit', metadata={'id': 0, 'relevance_score': 0.96604174}), Document(page_content='Answers to frequently asked questions about CUDA can be found at http://developer.nvidia.com/cuda-faq and in the CUDA Toolkit Release Notes.\\nReferences', metadata={'id': 2, 'relevance_score': 0.93817276})]\n"
          ]
        }
      ],
      "source": [
        "query = \"What is the NVIDIA CUDA Toolkit?\"\n",
        "result = chain(query)\n",
        "process_llm_response(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "imOU1OMUGITN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "89fce9de-1a0f-42a6-b529-f2da978f2f13"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The NVIDIA CUDA Toolkit can be installed on Windows using the following steps:\n",
            "\n",
            "1. Download the NVIDIA CUDA Toolkit from the official website: <https://developer.nvidia.com/cuda-downloads>\n",
            "2. Verify the download by comparing the MD5 checksum with the one provided on the website.\n",
            "3. Once the download is complete, run the installation executable (e.g., `cudaminer.exe` for 64-bit systems)\n",
            "to begin the installation process.\n",
            "4. Follow the on-screen instructions to complete the installation.\n",
            "\n",
            "Please note that the installation process may vary depending on your system configuration and preferences.\n",
            "Additionally, it's important to ensure that you have a CUDA-capable GPU installed in your system to take full\n",
            "advantage of the CUDA Toolkit.\n",
            "\n",
            "\n",
            "Sources: [Document(page_content='CUDA Installation Guide for Microsoft Windows\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n1. Introduction\\n1.1. System Requirements\\n1.2. About This Document\\n\\n\\n2. Installing CUDA Development Tools\\n2.1. Verify You Have a CUDA-Capable GPU\\n2.2. Download the NVIDIA CUDA Toolkit\\n2.3. Install the CUDA Software\\n2.3.1. Uninstalling the CUDA Software\\n\\n\\n2.4. Using Conda to Install the CUDA Software\\n2.4.1. Conda Overview\\n2.4.2. Installation\\n2.4.3. Uninstallation\\n2.4.4. Installing Previous CUDA Releases\\n\\n\\n2.5. Use a Suitable Driver Model\\n2.6. Verify the Installation\\n2.6.1. Running the Compiled Examples\\n\\n\\n\\n\\n3. Pip Wheels\\n4. Compiling CUDA Programs\\n4.1. Compiling Sample Projects\\n4.2. Sample Projects\\n4.3. Build Customizations for New Projects\\n4.4. Build Customizations for Existing Projects\\n\\n\\n5. Additional Considerations\\n6. Notices\\n6.1. Notice\\n6.2. OpenCL\\n6.3. Trademarks\\n\\n\\n\\n\\n\\n\\n\\n\\nInstallation Guide Windows\\n\\n\\n\\n\\n\\n ¬ª\\n1. Introduction\\n\\n\\n\\nv12.3 |\\nPDF\\n|\\nArchive\\n\\xa0\\n\\n\\n\\n\\n\\n\\nCUDA Installation Guide for Microsoft Windows\\nThe installation instructions for the CUDA Toolkit on Microsoft Windows systems.\\n\\n1. Introduction\\uf0c1\\nCUDA¬Æ is a parallel computing platform and programming model invented by NVIDIA. It enables dramatic increases in computing performance by harnessing the power of the graphics processing unit (GPU).\\nCUDA was developed with several design goals in mind:', metadata={'id': 1, 'relevance_score': 0.9661605}), Document(page_content='CUDA Toolkit Archive | NVIDIA Developer\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nCUDA Toolkit Archive \\n\\nPrevious releases of the CUDA Toolkit, GPU Computing SDK, documentation and developer drivers can be found using the links below. Please select the release you want from the list below, and be sure to check www.nvidia.com/drivers for more recent production drivers appropriate for your hardware configuration.\\n\\n\\xa0\\nDownload Latest CUDA Toolkit\\nLearn More about CUDA Toolkit', metadata={'id': 2, 'relevance_score': 0.9660385}), Document(page_content='2.6. Choose an Installation Method\\uf0c1\\nThe CUDA Toolkit can be installed using either of two different installation mechanisms: distribution-specific packages (RPM and Deb packages), or a distribution-independent package (runfile packages).\\nThe distribution-independent package has the advantage of working across a wider set of Linux distributions, but does not update the distribution‚Äôs native package management system. The distribution-specific packages interface with the distribution‚Äôs native package management system. It is recommended to use the distribution-specific packages, where possible.\\n\\nNote\\nFor both native as well as cross development, the toolkit must be installed using the distribution-specific installer. See the CUDA Cross-Platform Installation section for more details.\\n\\n\\n\\n2.7. Download the NVIDIA CUDA Toolkit\\uf0c1\\nThe NVIDIA CUDA Toolkit is available at https://developer.nvidia.com/cuda-downloads.\\nChoose the platform you are using and download the NVIDIA CUDA Toolkit.\\nThe CUDA Toolkit contains the CUDA driver and tools needed to create, build and run a CUDA application as well as libraries, header files, and other resources.\\nDownload Verification\\nThe download can be verified by comparing the MD5 checksum posted at https://developer.download.nvidia.com/compute/cuda/12.3.2/docs/sidebar/md5sum.txt with that of the downloaded file. If either of the checksums differ, the downloaded file is corrupt and needs to be downloaded again.\\nTo calculate the MD5 checksum of the downloaded file, run the following:\\nmd5sum <file>', metadata={'id': 0, 'relevance_score': 0.9603605})]\n"
          ]
        }
      ],
      "source": [
        "query = \"How can I install the NVIDIA CUDA Toolkit on windows?\"\n",
        "result = chain(query)\n",
        "process_llm_response(result)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"Can you give me some best practices using the NVIDIA CUDA Toolkit?\"\n",
        "result = chain(query)\n",
        "process_llm_response(result)"
      ],
      "metadata": {
        "id": "qdE6h7L4DEIB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d6573d06-a83f-427c-f609-97c87ee65274"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I'm happy to help! Here are some best practices for using the NVIDIA CUDA Toolkit:\n",
            "\n",
            "1. Familiarize yourself with the CUDA programming model and the C++ syntax for accessing the GPU. This will\n",
            "make it easier to write efficient and correct code.\n",
            "2. Use the CUDA debugger to identify and fix issues in your code.\n",
            "3. Take advantage of the various development tools provided with the CUDA Toolkit, such as NVIDIA¬Æ Nsight‚Ñ¢\n",
            "Eclipse Edition, NVIDIA Visual Profiler, CUDA-GDB, and CUDA-MEMCHECK. These tools can help you optimize your\n",
            "code and identify performance bottlenecks.\n",
            "4. Keep your code simple and easy to read. Avoid complex loops and data structures that can be difficult to\n",
            "parallelize.\n",
            "5. Use the CUDA Streams API to split your code into multiple streams and execute them simultaneously on\n",
            "multiple GPU cores. This can greatly improve performance on multi-GPU systems.\n",
            "6. Use the CUDA shared memory and constant memory to store data that is used frequently across multiple\n",
            "threads. This can reduce the amount of memory traffic and improve performance.\n",
            "7. Profile your code regularly to identify areas where performance can be improved. Use the CUDA Performance\n",
            "Analyzer to measure the performance of your code and identify bottlenecks.\n",
            "8. Take advantage of the CUDA GPU occupancy calculator to understand how much of your GPU resources are being\n",
            "utilized. This can help you optimize your code for better performance.\n",
            "9. Use the CUDA compiler options to control the optimization level and generate more efficient code.\n",
            "10. Keep your code up to date with the latest CUDA toolkit releases to take advantage of new features and\n",
            "improvements.\n",
            "\n",
            "By following these best practices, you can write efficient and correct CUDA code that takes full advantage of\n",
            "the parallel processing capabilities of the GPU.\n",
            "\n",
            "\n",
            "Sources: [Document(page_content='16. Additional Considerations\\uf0c1\\nNow that you have CUDA-capable hardware and the NVIDIA CUDA Toolkit installed, you can examine and enjoy the numerous included programs. To begin using CUDA to accelerate the performance of your own applications, consult the CUDA C++ Programming Guide, located in /usr/local/cuda-12.0/doc.\\nA number of helpful development tools are included in the CUDA Toolkit to assist you as you develop your CUDA programs, such as NVIDIA¬Æ Nsight‚Ñ¢ Eclipse Edition, NVIDIA Visual Profiler, CUDA-GDB, and CUDA-MEMCHECK.\\nFor technical support on programming questions, consult and participate in the developer forums at https://forums.developer.nvidia.com/c/accelerated-computing/cuda/206.\\n\\n\\n17. Switching between Driver Module Flavors\\uf0c1\\nUse the following steps to switch between the NVIDIA driver legacy and open module flavors on your system.\\n\\nNote\\nIf switching to open module, experimental support for GeForce and Quadro SKUs can be enabled with:\\necho \"options nvidia NVreg_OpenRmEnableUnsupportedGpus=1\" | sudo tee /etc/modprobe.d/nvidia-gsp.conf\\n\\n\\n\\n\\nNote\\nReplace XXX with the NVIDIA driver branch number such as 515 or 520.\\n\\nFedora, RHEL 9 / Rocky Linux 9, RHEL 8 / Rocky Linux 8\\nTo switch from legacy to open:\\nsudo dnf module switch-to nvidia-driver:XXX-open --allowerasing\\n\\n\\nTo switch from open to legacy:\\nsudo dnf module switch-to nvidia-driver:XXX-dkms --allowerasing\\n\\n\\nKylin OS\\nTo switch between legacy and open: uninstall, then reinstall.\\nUbuntu\\nTo switch from legacy to open:\\nsudo apt-get --purge remove nvidia-kernel-source-XXX\\nsudo apt-get install --verbose-versions nvidia-kernel-open-XXX\\nsudo apt-get install --verbose-versions cuda-drivers-XXX\\n\\n\\nTo switch from open to legacy:\\nsudo apt-get remove --purge nvidia-kernel-open-XXX\\nsudo apt-get install --verbose-versions cuda-drivers-XXX', metadata={'id': 2, 'relevance_score': 0.96733886}), Document(page_content='The discussions in this guide all use the C++ programming language, so you should be comfortable reading C++ code.\\nThis guide refers to and relies on several other documents that you should have at your disposal for reference, all of which are available at no cost from the CUDA website https://docs.nvidia.com/cuda/. The following documents are especially important resources:\\n\\nCUDA Installation Guide\\nCUDA C++ Programming Guide\\nCUDA Toolkit Reference Manual\\n\\nIn particular, the optimization section of this guide assumes that you have already successfully downloaded and installed the CUDA Toolkit (if not, please refer to the relevant CUDA Installation Guide for your platform) and that you have a basic familiarity with the CUDA C++ programming language and environment (if not, please refer to the CUDA C++ Programming Guide).\\n\\n\\n\\n1.3. Assess, Parallelize, Optimize, Deploy\\uf0c1\\n\\nThis guide introduces the Assess, Parallelize, Optimize, Deploy(APOD) design cycle for applications with the goal of helping application developers to rapidly identify the portions of their code that would most readily benefit from GPU acceleration, rapidly realize that benefit, and begin leveraging the resulting speedups in production as early as possible.\\nAPOD is a cyclical process: initial speedups can be achieved, tested, and deployed with only minimal initial investment of time, at which point the cycle can begin again by identifying further optimization opportunities, seeing additional speedups, and then deploying the even faster versions of the application into production.\\n\\n\\n\\n\\n\\n1.3.1. Assess\\uf0c1', metadata={'id': 0, 'relevance_score': 0.9631049}), Document(page_content='Answers to frequently asked questions about CUDA can be found at http://developer.nvidia.com/cuda-faq and in the CUDA Toolkit Release Notes.\\nReferences', metadata={'id': 1, 'relevance_score': 0.9391717})]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What is nvflare?\"\n",
        "result = chain(query)\n",
        "process_llm_response(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JyLp4igpu4dp",
        "outputId": "c580d237-02a8-4b2c-fc5a-ab4c851e9c16"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NvFlare is a Python package for creating and manipulating video effects using NVIDIA's VFX (Visual Effects)\n",
            "API. It allows developers to create complex video effects using a variety of built-in filters and models, as\n",
            "well as custom filters and models through the use of Python scripts.\n",
            "\n",
            "\n",
            "Sources: [Document(page_content='Once venv is installed, you can use it to create a virtual environment with:\\n$ python3 -m venv nvflare-env\\n\\n\\nThis will create the nvflare-env directory in current working directory if it doesn‚Äôt exist,\\nand also create directories inside it containing a copy of the Python interpreter,\\nthe standard library, and various supporting files.\\nActivate the virtualenv by running the following command:\\n$ source nvflare-env/bin/activate\\n\\n\\nYou may find that the pip and setuptools versions in the venv need updating:\\n(nvflare-env) $ python3 -m pip install -U pip\\n(nvflare-env) $ python3 -m pip install -U setuptools\\n\\n\\n\\n\\n\\nInstall Stable Release¬∂\\nStable releases are available on NVIDIA FLARE PyPI:\\n$ python3 -m pip install nvflare\\n\\n\\n\\nNote\\nIn addition to the dependencies included when installing nvflare, many of our example applications have additional packages that must be installed.\\nMake sure to install from any requirement.txt files before running the examples.\\nSee nvflare/app_opt for modules and components with optional dependencies.', metadata={'id': 0, 'relevance_score': 0.96731913}), Document(page_content='The NvCVImage structure that will be used as the output of the effect.\\nBecause no effect has more than one output image, this selector is equivalent to NVVFX_OUTPUT_IMAGE. \\n\\n\\n            NVVFX_MODEL_DIRECTORY \"ModelDir\"\\n           \\n\\n            The path to the folder that contains the model files that will be used for the transformation. \\n           \\n\\n            NVVFX_CUDA_STREAM \"CudaStream\"\\n           \\n\\n            The CUDA stream in which to run the video effect filter.\\n           \\n\\n            NVVFX_CUDA_GRAPH ‚ÄúCudaGraph‚Äù\\n           \\n\\n            Enables CUDA Graph Optimization. \\n           \\n\\n            NVVFX_INFO \"Info\"\\n           \\n\\n            Get information about a video effect filter and its parameters.\\n           \\n\\n            NVVFX_MAX_INPUT_WIDTH \"MaxInputWidth\"\\n           \\n\\n            Maximum width of the supported input.\\n           \\n\\n            NVVFX_MAX_INPUT_HEIGHT \"MaxInputHeight\"\\n           \\n\\n            Maximum height of the supported input.\\n           \\n\\n            NVVFX_MAX_NUMBER_STREAMS \"MaxNumberStreams\" \\n           \\n\\n            Maximum number of concurrent input streams.\\n           \\n\\n            NVVFX_SCALE \"Scale\"\\n           \\n\\n            Scale factor. This is used to scale the values of the images during transfer to match formats in a pipeline of effects. \\n           \\n\\n            NVVFX_STRENGTH \"Strength\"\\n           \\n\\n            Strength for the filters that use this parameter. Higher strength implies a stronger effect. \\n           \\n\\n            NVVFX_STRENGTH_LEVELS \"StrengthLevels\"\\n           \\n\\n            Number of unique strength levels in the interval [0, 1]. Currently this only applies to the Webcam Denoise filter, and is set to 2, which implies that the two strength levels are 0 or 1. \\n           \\n\\n            NVVFX_MODE \"Mode\"\\n           \\n\\n            The mode of an AI green screen, Artifact resolution, and Super resolution filter. \\n\\n0: Quality mode \\n1: Performance mode \\n\\n\\n\\n            NVVFX_TEMPORAL \"Temporal\"', metadata={'id': 2, 'relevance_score': 0.9429633}), Document(page_content='Supported NVAPI Functions', metadata={'id': 1, 'relevance_score': 0.058123946})]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What is the difference between NVIDIA's BioMegatron and Megatron 530B LLM?\"\n",
        "result = chain(query)\n",
        "process_llm_response(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qcv-nw30vWwO",
        "outputId": "d45bd731-c272-4671-ae72-f35c541dbf5e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NVIDIA's BioMegatron and Megatron 530B LLM are both large language models (LLMs) developed by NVIDIA. However,\n",
            "they differ in their architecture and capabilities.\n",
            "\n",
            "BioMegatron is a transformer-based LLM that uses a combination of CPU and GPU resources to achieve state-of-\n",
            "the-art results in various natural language processing (NLP) tasks. It was specifically designed for bio-\n",
            "related tasks such as protein structure prediction, drug discovery, and genomics analysis. BioMegatron has 530\n",
            "billion parameters and is trained on a dataset of over 1 exabyte of text data.\n",
            "\n",
            "On the other hand, Megatron 530B is also a transformer-based LLM that is designed for general-purpose language\n",
            "understanding and generation tasks. It has 530 billion parameters and is trained on a diverse dataset of text\n",
            "from the internet. While Megatron 530B can be used for a wide range of NLP tasks, it may not be as specialized\n",
            "or optimized for bio-related tasks as BioMegatron.\n",
            "\n",
            "In summary, while both BioMegatron and Megatron 530B are powerful LLMs, BioMegatron is specifically designed\n",
            "for bio-related tasks and has more specialized training data, while Megatron 530B is a more general-purpose\n",
            "LLM that can be used for a wider range of NLP tasks.\n",
            "\n",
            "\n",
            "Sources: [Document(page_content='nlp-megatron10\\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan\\xa0N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. 2023. arXiv:1706.03762.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nprevious\\nMultimodal API\\n\\n\\n\\n\\nnext\\nMigrating from Megatron-LM\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n Contents\\n  \\n\\n\\nReferences\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nBy NVIDIA CORPORATION\\n\\n\\n\\n\\n    \\n      ¬© Copyright ¬© 2021-2023 NVIDIA Corporation & Affiliates. All rights reserved..\\n      \\n\\n\\n\\n\\n  Last updated on Feb 28, 2024.', metadata={'id': 1, 'relevance_score': 0.96725535}), Document(page_content='NVIDIA Grace Hopper Superchip and NVLink Switch System are the building blocks of the NVIDIA DGX GH200 architecture. NVIDIA Grace Hopper Superchip combines the Grace and Hopper architectures using NVIDIA NVLink-C2C to deliver a CPU + GPU coherent memory model. The NVLink Switch System, powered by the fourth generation of NVLink technology, extends NVLink connection across superchips to create a seamless, high-bandwidth, multi-GPU system.\\nEach NVIDIA Grace Hopper Superchip in NVIDIA DGX GH200 has 480 GB LPDDR5 CPU memory, at an eighth of the power per GB, compared with DDR5 and 96 GB of fast HBM3. NVIDIA Grace CPU and Hopper GPU are interconnected with NVLink-C2C, providing 7x more bandwidth than PCIe Gen5 at one-fifth the power. \\nThe NVLink Switch System forms a two-level, non-blocking, fat-tree NVLink fabric to connect 256 Grace Hopper Superchips in a DGX GH200 system fully. Every GPU in DGX GH200 can access the memory of other GPUs and extended GPU memory of all NVIDIA Grace CPUs at 900 GBps.\\xa0\\nCompute baseboards hosting Grace Hopper Superchips are connected to the NVLink Switch System using a custom cable harness for the first layer of NVLink fabric. LinkX cables extend the connectivity in the second layer of NVLink fabric.\\xa0\\nFigure 2. Topology of a fully connected NVIDIA NVLink Switch System across NVIDIA DGX GH200 consisting of 256 GPUs\\nIn the DGX GH200 system, GPU threads can address peer HBM3 and LPDDR5X memory from other Grace Hopper Superchips in the NVLink network using an NVLink page table. NVIDIA Magnum IO acceleration libraries optimize GPU communications for efficiency, enhancing application scaling with all 256 GPUs.', metadata={'id': 0, 'relevance_score': 0.95699626}), Document(page_content='BioNeMo | Generative AI Platform | NVIDIA\\n\\n\\n\\n\\n\\n\\n\\n\\nNVIDIA Home\\nNVIDIA Home\\n\\n\\n\\n\\n\\nMenu\\nMenu icon\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nMenu\\nMenu icon\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nClose\\nClose icon\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nClose\\nClose icon\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nClose\\nClose icon\\n\\n\\n\\n\\n\\nCaret down icon\\nAccordion is closed, click to open.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nCaret down icon\\nAccordion is closed, click to open.\\n\\n\\n\\n\\n\\n\\n\\nCaret up icon\\nAccordion is open, click to close.\\n\\n\\n\\n\\n\\n\\n\\nCaret right icon\\nClick to expand\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nCaret right icon\\nClick to expand\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nCaret right icon\\nClick to expand menu.\\n\\n\\n\\n\\n\\n\\n\\nCaret left icon\\nClick to collapse menu.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nCaret left icon\\nClick to collapse menu.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nCaret left icon\\nClick to collapse menu.\\n\\n\\n\\n\\n\\n\\n\\nShopping Cart\\nClick to see cart items\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSearch icon\\nClick to search\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSkip to main content\\n\\n\\n \\nArtificial Intelligence Computing Leadership from NVIDIA\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nMain Menu\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nProducts\\n\\n\\n\\n\\n\\n\\nHardware\\n\\n\\nSoftware\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nGaming and Creating\\n\\n\\nGeForce Graphics Cards\\n\\n\\nLaptops\\n\\n\\nG-SYNC Monitors\\n\\n\\nStudio\\n\\n\\nSHIELD TV\\n\\n\\n\\n\\nLaptops and Workstations\\n\\n\\nLaptops\\n\\n\\nNVIDIA RTX in Desktop Workstations\\n\\n\\nNVIDIA RTX in Professional Laptops\\n\\n\\nNVIDIA RTX-Powered AI Workstations\\n\\n\\n\\n\\nCloud and Data Center\\n\\n\\nOverview\\n\\n\\nGrace CPU\\n\\n\\nDGX Platform\\n\\n\\nEGX Platform\\n\\n\\nIGX Platform\\n\\n\\nHGX Platform\\n\\n\\nNVIDIA MGX\\n\\n\\nNVIDIA OVX\\n\\n\\nDRIVE Sim\\n\\n\\n\\n\\nNetworking\\n\\n\\nOverview\\n\\n\\nDPUs and SuperNICs\\n\\n\\nEthernet\\n\\n\\nInfiniBand\\n\\n\\n\\n\\nGPUs\\n\\n\\nGeForce\\n\\n\\nNVIDIA RTX / Quadro\\n\\n\\nData Center\\n\\n\\n\\n\\nEmbedded Systems\\n\\n\\nJetson\\n\\n\\nDRIVE AGX\\n\\n\\nClara AGX\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nApplication Frameworks\\n\\n\\nAI Inference - Triton\\n\\n\\nAutomotive - DRIVE\\n\\n\\nCloud-AI Video Streaming - Maxine\\n\\n\\n\\nComputational Lithography - cuLitho\\n\\n\\n\\nCybersecurity - Morpheus\\n\\n\\n\\nData Analytics - RAPIDS\\n\\n\\nHealthcare - Clara\\n\\n\\nHigh-Performance Computing\\n\\n\\n\\nIntelligent Video Analytics - Metropolis\\n\\n\\nLarge Language Models - NeMo Framework\\n\\n\\nMetaverse Applications - Omniverse', metadata={'id': 2, 'relevance_score': 0.9503217})]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"How to find the JetPack version of NVIDIA Jetson Device?\"\n",
        "result = chain(query)\n",
        "process_llm_response(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ckWEfioCijTo",
        "outputId": "4b0da95d-cf57-4f79-b360-de6d607f553f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "To find the JetPack version of an NVIDIA Jetson device, you can follow these steps:\n",
            "\n",
            "1. Connect to the device's terminal or command prompt using a serial cable or remotely via SSH.\n",
            "2. Run the following command to check the JetPack version: `jetpack_version`\n",
            "\n",
            "This command will display the current version of the JetPack software installed on the device.\n",
            "\n",
            "Alternatively, you can check the version of the Jetson device by running the following command:\n",
            "`jetson_version`\n",
            "\n",
            "This command will display the version number of the Jetson device, which can be used to identify the JetPack\n",
            "version installed on the device.\n",
            "\n",
            "\n",
            "Sources: [Document(page_content='NVIDIA JetPack Documentation\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nNVIDIANVIDIA JetPack Documentation\\n\\nSearch In:\\nEntire Site\\nJust This Document\\nclear search\\nsearch\\n\\n\\n\\nNVIDIA JetPack SDK\\n\\nIntroduction to JetPack\\n\\nRelease Notes\\n\\nRelease Notes\\n\\nInstallation and Setup\\n\\nHow to Install JetPack\\n\\nCopyright And License Notices\\n\\nJetPack EULA\\n\\n\\n\\n\\nSearch Results\\n\\n\\n\\n\\nNVIDIA JetPack SDK\\n\\n\\nIntroduction to JetPack\\nUse JetPack to flash your Jetson Developer Kit with the latest OS image, to install developer tools for both Linux host PC\\n                     and developer kit, and install the libraries and APIs, samples, and documentation needed to jumpstart your development environment.\\n                  \\n\\nRelease Notes\\n\\n\\nRelease Notes\\nSee the latest features and updates for this version of NVIDIA JetPack.\\n\\nInstallation and Setup\\n\\n\\nHow to Install JetPack\\nThis section contains the instructions for installing and setting up NVIDIA JetPack.\\n\\nCopyright And License Notices\\n\\n\\nJetPack EULA\\nThis document is the End User License Agreement (EULA) for NVIDIA JetPack. This document\\n                     contains specific license terms and conditions for NVIDIA JetPack. By accepting this\\n                     agreement, you agree to comply with all the terms and conditions applicable to the\\n                     specific product(s) included herein.', metadata={'id': 0, 'relevance_score': 0.9664102}), Document(page_content='NVIDIA JetPack Documentation\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nNVIDIA JetPack SDK\\n\\nIntroduction to JetPack\\nRelease Notes\\n\\nInstallation and Setup\\n\\nHow to Install JetPack\\n\\nCopyright And License Notices\\n\\nJetPack EULA\\n\\n\\n\\n\\nSearch Results\\n\\n\\n\\n\\nNVIDIA JetPack Documentation\\n\\n\\nSearch In:\\nEntire Site\\nJust This Document\\n\\n\\n\\n\\nNVIDIA JetPack SDK\\n\\n\\nIntroduction to JetPack\\nNVIDIA JetPack SDK is the most comprehensive solution for building AI applications. It includes the latest OS images for Jetson\\n                     products, along with libraries and APIs, samples, developer tools, and documentation.\\n                  \\nRelease Notes\\nSee the latest features and updates for this version of NVIDIA JetPack.\\n\\nInstallation and Setup\\n\\n\\nHow to Install JetPack\\nThis section contains the instructions for installing and setting up NVIDIA JetPack.\\n\\nCopyright And License Notices\\n\\n\\nJetPack EULA\\nThis document is the End User License Agreement (EULA) for NVIDIA JetPack. This document\\n                     contains specific license terms and conditions for NVIDIA JetPack. By accepting this\\n                     agreement, you agree to comply with all the terms and conditions applicable to the\\n                     specific product(s) included herein.', metadata={'id': 2, 'relevance_score': 0.9621468}), Document(page_content='Jetson Linux API Reference: Main Page | NVIDIA Docs\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nJetson Linux API Reference\\n\\n\\n\\r\\n                32.7.3 Release\\r\\n            \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nJetson Linux API Reference Documentation', metadata={'id': 1, 'relevance_score': 0.8221688})]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "cWysnfP7q2Km"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "zco0N-FAjsnY",
        "kOcDyMZUjiQ4",
        "0tieAjP9jDbK"
      ],
      "gpuType": "T4",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "3c0d074fff844a2b8fb7568dcfe62c40": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_21244bf9935c4dae9d72b12af10522f2",
              "IPY_MODEL_fc0a5a0f773548d593df0fcf88a279e3",
              "IPY_MODEL_47c0a3a39c8d402aa72a44c6b0b1be0f"
            ],
            "layout": "IPY_MODEL_bef30a56c8474246b8c666bc1186c3ba"
          }
        },
        "21244bf9935c4dae9d72b12af10522f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f47bcd669bb14efca6616f6e08d0db66",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_5bae4b8e70cb4db9a82a222ca8323be6",
            "value": "Loading‚Äácheckpoint‚Äáshards:‚Äá100%"
          }
        },
        "fc0a5a0f773548d593df0fcf88a279e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_229ac37ba865425da35d62723787aadd",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_cf5554ecca14424b8296d33926e1b61a",
            "value": 2
          }
        },
        "47c0a3a39c8d402aa72a44c6b0b1be0f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_61d341f3cc084c77ae8685eb1341a3ef",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_dcdd2fcef1a9411ebe2b335fdc7c56e6",
            "value": "‚Äá2/2‚Äá[00:06&lt;00:00,‚Äá‚Äá2.76s/it]"
          }
        },
        "bef30a56c8474246b8c666bc1186c3ba": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f47bcd669bb14efca6616f6e08d0db66": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5bae4b8e70cb4db9a82a222ca8323be6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "229ac37ba865425da35d62723787aadd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cf5554ecca14424b8296d33926e1b61a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "61d341f3cc084c77ae8685eb1341a3ef": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dcdd2fcef1a9411ebe2b335fdc7c56e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}